{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency in _The Machine Stops_ by E.M. Forster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to install a new library and download some files to support it. Run the following cell, and a few moments later you should see the 'NLTK Downloader' window pop up. Select 'book' and click download. It will take a couple of minutes to download while you follow the next couple of steps, but you will need to check it has finished and close the window before you can run cells in your notebook again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://ia800501.us.archive.org/15/items/LibrivoxCdCoverArt10/Machine_Stops_1108.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercise in this notebook is based on the short story \"The Machine Stops\" (1909) by E.M. Forster, and the article [\"Where the Machine Stops: Software as Reader and the Rise of New Literatures\"](http://canterbury.summon.serialssolutions.com/#!/search?bookMark=ePnHCXMwXV09C8IwFCzSQe1_kAfOhbb5aOMqioMu1cGtpGkCQkExgn_fS60dnPPIknD3jnfcW0YxdKudhWEuZ2nBeBUPyYgcmqe8zif7hVLFIqr3IXJgQ0CepyW0PHQaPISWzq_7w2_oDDB6a5xpT_Vg7iUo7KGyvnlLd0dAAzpOGcQ-iS773WV7SMdlAukDnJZ20mjhXNYpyQWzHMSbuVIaMJax4FjbAjG0zFoJyNbadS2EkzFhjKeEM5ol0fp77Q-xm_GxfZNLEfqaSoWdR6u_qq7vGwbVU-Gni5x9ABKuUmM) (2015) by Tom Lynch. Lynch uses [Voyant Tools](http://voyant-tools.org/) to visualise a word frequency list for \"The Machine Stops\". We're going to reproduce his approach using Python and Jupyter notebooks, as well as explore some discourse analysis tools from the Natural Language Tool Kit (NLTK)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get a copy of the text. Hopefully you've read it already. We'll use the wikisource text here: https://en.wikisource.org/wiki/The_Machine_Stops You can access it in the IFrame below (IFrames are a handy way to embed other web pages into your notebooks).\n",
    "\n",
    "- To download the file in a text format, go to the 'Choose Format' link under the section headed 'Download/Print' on the left sidebar. \n",
    "- This will take you to a form where you can enter the page title ('The_Machine_Stops') and select 'txt' for the file format. Do not include fonts. \n",
    "- Save the downloaded file in the same directory as this notebook. It should appear in your list of files in the 'Home' tab in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://en.wikisource.org/wiki/The_Machine_Stops', width=1000, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's take a look at the text. \n",
    "\n",
    "We could open the file in a separate text editor (such as TextWrangler on a Mac), but it's useful to know about the notebook's magic commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%less The_Machine_Stops.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the next cell, the file has some material like title, contents, and license statement that we don't want to include in our word counts. Before we continue, \n",
    "- Go to the 'Home' tab in your browser\n",
    "- open 'The_Machine_Stops.txt' and remove everything before the first chapter title ('The Air Ship').  \n",
    "- Also remove the extra material from the end of the story.  \n",
    "- Go to File > Save to make sure your changes are saved.\n",
    "- Once back in your notebook, use ```%less The_Machine_Stops.txt``` again to confirm your edits look right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great - you now have your text available. However, before we can count terms appearing in the text, we need to convert it to a more convenient structure. In Natural Language Processing parlance, this is called 'tokenisation'. We'll create a list of tokens using a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('The_Machine_Stops.txt') as f:\n",
    "    text = f.read()\n",
    "    word_list = []\n",
    "    for word in text.split():\n",
    "        word_list.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```with``` keyword is useful for opening files, as it automatically closes the file afterwards for us. Here, the text file gets opened, and assigned to a variable 'f' - a convention for 'file'. The file is parsed into memory using ```read()``` and assigned to a variable ```text```\n",
    "\n",
    "A ```word_list``` variable is created, which lists all the words in the text. Notice the pattern of creating an empty list, then iterating to add each word to the list. By default the ```split()``` string method uses whitespace to separate each word, as we've seen previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the first 30 words in the list.\n",
    "word_list[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a rough list of words. How would we count how many times each word appears using the Python we've learned so far? We could use a Python dictionary to create a record of each word and the number of times it appears in the text. It will look something like:\n",
    "\n",
    "```frequencies = {'The': 102), 'Air': 9, 'Ship': 6), 'Imagine,': 5 ...}```\n",
    "\n",
    "Pseudo code for the process we'll follow would be:\n",
    "- Create word_dictionary as an empty dictionary\n",
    "- Loop over each word in the existing ```word_list```\n",
    "- If it doesn't exist in ```freqencies```, create an entry for the word and give it a value of 1 \n",
    "- If it does exist, add one to the value (ie the count for that word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequencies = {}\n",
    "for word in word_list:\n",
    "    if word in frequencies.keys():\n",
    "        frequencies[word] = frequencies[word] + 1\n",
    "    else:\n",
    "        frequencies[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see this pattern for processing text data a lot. It is so common it has a name: the _accumulator pattern_. It can have more steps but generally starts with an empty list or dictionary, then loops through existing data. If some condition is satisfied, it adds to our new list or dictionary, perhaps appending an item or, as in this case, increasing the count for a particular word by 1. Either way, we're accumulating some new data at each run through the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's see what frequencies looks like\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's find the entry for 'machine'\n",
    "# frequencies is a dictionary of key-value pairs. We get a single value by selecting the key\n",
    "frequencies['machine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one instance of 'machine'? Something is wrong! Look through the output of ```print(frequencies)``` above and explain what is going wrong. You can also use your browser's find/replace to search for words, eg 'book' and 'Book'. In the cell below, explain how we should address this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your notes here..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: When you're finished, you may want to clear long output cells, such as the output of ```print(frequencies)``` above. To do this, select the cell and go to Cell > Current Outputs > Clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Typically we need to 'clean' our text to make the results of the processing useful. To start with, let's assume we want to remove all punctuation and capitalisation. Write a simple loop that employs the string methods [```lower()```](https://docs.python.org/3/library/stdtypes.html#str.lower) and [```strip()```](https://docs.python.org/3/library/stdtypes.html#str.strip), along with the list method ```append()``` to achieve this. You'll need to write three new lines of code in the cell below, one under each commented instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_word_list = []\n",
    "\n",
    "for word in word_list:\n",
    "    # lowercase each word\n",
    "    lowercase_word = word.lower()\n",
    "    \n",
    "    # strip punctuation and extra whitespace around each word\n",
    "    # pass some punctuation characters as an argument to the strip() method below,eg strip('?!\"-,.')\n",
    "    # run the cell to check how well this works\n",
    "    clean_word = lowercase_word.strip('')\n",
    "    \n",
    "    # ensure clean_word is not empty. This removes all empty strings. != means 'not equal to'\n",
    "    if clean_word != '':\n",
    "        # append each cleaned word to clean_word_list\n",
    "        clean_word_list.append(clean_word)\n",
    "    \n",
    "clean_word_list[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should find many more instances of the word 'machine'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequencies = {}\n",
    "for word in clean_word_list:\n",
    "    if word in frequencies.keys():\n",
    "        frequencies[word] = frequencies[word] + 1\n",
    "    else:\n",
    "        frequencies[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequencies['machine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Natural Language Toolkit (NLTK) library to make this easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ok, before we go further, let's look at a better way to make progress here. While we could write most of the elements of a word frequency program using the Python we know already, the NLTK library will help us do things a lot faster and more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('The_Machine_Stops.txt') as f:\n",
    "    raw_text = f.read()\n",
    "    tokens = nltk.word_tokenize(raw_text)\n",
    "    text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the ```with``` and ```as``` keywords to open a file. This is the recommended way to open files in Python. You can also open files in a more sequential way, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = open('The_Machine_Stops.txt')\n",
    "raw_text = file.read()\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "text = nltk.Text(tokens)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second example, we open the file, read it, make our tokens and our text variables, then close the file. This works just fine, but you'll notice in the first example you don't have to remember to close the file - the ```with``` block does this for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why using pre-existing libraries is a great idea! We've written fewer lines of code than our efforts above, but we've already got a lot of methods available to us. What's more, NLTK will look after the issues of capitalisation and punctuation for us when these methods are used.\n",
    "\n",
    "For example, we can quickly build a concordance for any word using the ```concordance()``` method of the text object. Notice how we don't need to specify the uppercase 'M' - NLTK will find both cases for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.concordance('machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice we find many more instances of the word machine using NLTK. Think about what you found interesting aobut this story, and substitute some relevant words into the concordance method above. What do you notice? Make some notes about what you found in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's Frequency Distribution object, called ```FreqDist()``` will be useful for getting sorted lists of word frequencies, which we can inspect and update easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(text)\n",
    "freq_dist.most_common(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a sorted frequency list, with each item an ordered pair consisting of the token and its freqency. These items are acutally another data type we haven't seen yet, called tuples. They are a bit like a list, and offer certain advantages over lists (eg performance), but they can't be modified like a list. \n",
    "\n",
    "In any case, we want a good way to filter out the tokens that aren't of interest. Looking at Tom Lynch's most frequent words table (p301 of the article), it is clear he has filtered a lot of function words. NLTK provides a flexible way to do this, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "filtered_words = []\n",
    "for word in word_list:\n",
    "    if word not in stopwords.words('english'):\n",
    "        lower_word = word.lower()\n",
    "        remove_punc = lower_word.strip(',.;:-?!\\'\\\"')\n",
    "        if remove_punc != '':\n",
    "            filtered_words.append(remove_punc)\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_text = nltk.Text(filtered_words)\n",
    "filtered_freq_dist = nltk.FreqDist(filtered_text)\n",
    "filtered_freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking much cleaner, but it is still nowhere as stringently filtered as Lynch's list. Do you think Lynch gives enough detail when he suggests he's presenting the \"Top 10 most frequently used words\" in the story (p301)? Give reasons for your view in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your reasons here..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to filter our words to the degree Lynch has done, we can achieve this using Part of Speech tags. Part of Speech tagging is a process that identifies the linguistic part of speech of every word in a text. It is a process that linguists and computer scientists have developed to the point where it can be very accurate. NLTK may not be the best POS tagger, but it will do for our purposes.\n",
    "\n",
    "We'll consult [a list](http://www.nltk.org/book/ch05.html#tab-universal-tagset) of abbreviations for parts of speech using the Universal tagset to construct our new filtered list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_filtered_words = []\n",
    "accepted_tags = ['NOUN']\n",
    "\n",
    "# use the nltk.pos_tag method, giving it the list stored in 'tokens'\n",
    "tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "\n",
    "for tag in tags:\n",
    "    if tag[1] in accepted_tags:\n",
    "        pos_filtered_words.append(tag[0].lower())\n",
    "\n",
    "pos_filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_filtered_text = nltk.Text(pos_filtered_words)\n",
    "pos_filtered_freq_dist = nltk.FreqDist(pos_filtered_text)\n",
    "pos_filtered_freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this isn't exactly the same as Lynch's list (which kinds of words are missing?), it will do for our purposes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly, we start to need some tricky looking Python to do the next step, but if you go through it slowly it should be possible to see what is going on. You may wish to refer to the Python documentation for the ```range()``` function to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first we define the size of our chunks / segments - Lynch uses tenths\n",
    "from math import ceil\n",
    "chunk_size = ceil(len(pos_filtered_words) / 10)\n",
    "# ceil, short for ceiling, takes a float (eg 6.45) and rounds it up to an integer (6.45 becomes 7)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "# create a loop starting at 0, stopping at len(pos_filtered_words), and stepping up in n amounts\n",
    "for i in range(0, len(pos_filtered_words), chunk_size):\n",
    "    chunks.append(pos_filtered_words[i:i+chunk_size]) \n",
    "\n",
    "print('chunk_size is equal to', chunk_size)\n",
    "print('number of chunks is', len(chunks))\n",
    "chunks\n",
    "# now chunks is a list with 10 lists inside it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find distributions of our words of interest for each chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to know how words of interest (like 'machine') are distributed across chunks. This will help us to see where particular terms are used within the text, and make visual comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "machine = []\n",
    "for i in range(10):\n",
    "    machine.append(chunks[i].count('machine'))\n",
    "machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "book = []\n",
    "for i in range(10):\n",
    "    book.append(chunks[i].count('book'))\n",
    "book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the graph with Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib is a plotting library for Python. It can be instructed to create plots 'inline' within a Jupyter notebook using the cell magic ```%matplotlib inline```. We've already defined our data giving distributions of the words 'machine' and 'book' above, so once we've imported matplotlib's pyplot module, all we have to do is create 10 segments, and add our data to the plot. As you can see, we can also easily add labels, a title, and a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "segments = [1,2,3,4,5,6,7,8,9,10]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(machine)\n",
    "plt.plot(book)\n",
    "plt.xlabel('Story Chunks')\n",
    "plt.ylabel(\"Raw Frequencies\")\n",
    "plt.title(\"Word frequency comparison of 'machine' and 'book' in Forster's The Machine Stops\")\n",
    "plt.legend(['Machine', 'Book'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now choose some other pairs of words and copy the code above into some new cells to create new plots below. Can you find other interesting patterns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another great took NLTK provides is a dispersion plot. This can be created from an nltk.Text object, such as ```pos_filtered_text``` which we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_filtered_text.dispersion_plot(['machine', 'book'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us some of the same information as the line plot, but of course it doesn't let us see the 'overlap' where the frequency of the word machine dips below that of the word book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating a dispersion plot of some pairs of words you compared earlier in the frequency plot. What information is visible in the lexical dispersion plot that is not visible in the frequency plot? Do you notice any patterns that you can relate back to the story?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll come back to explore more powerful features offered by NLTK a bit later. For now, we want to be able to count the words, plot frequencies and see dispersion plots of any words we're particularly interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing up\n",
    "\n",
    "Don't forget to save your notebook before closing your browser window. Note you can download your notebook file in a range of formats by going to File > Download as. This might be useful if you want to keep a record of the lab as a static document, or you want to print your work."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
