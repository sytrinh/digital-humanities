{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Recipes for Assignment 1\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "The documentation for TextBlob, if you're interested is [here](https://textblob.readthedocs.io/en/dev/)  \n",
    "TF-IDF code adapted from Steven Loria: http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/  \n",
    "Explanations for the code below are provided. Feel free to modify the code to try out ideas, but doing so is not required for assignment 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1918ebc2ccb0.txt\n",
      "2361705f299d.txt\n",
      "38a6c08d4652.txt\n",
      "3ebb50ba370e.txt\n",
      "3f9d20f019a3.txt\n",
      "6312bfd4c87a.txt\n",
      "89b97cb1cca5.txt\n",
      "8ebd88efcff6.txt\n",
      "b054a3b4fb16.txt\n",
      "bde914224749.txt\n",
      "becf40fd76b9.txt\n",
      "efca757f0313.txt\n",
      "f611db9ba568.txt\n",
      "fa5a1fd08da4.txt\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this cell we load a directory of text files into memory using the TextBlob library.\n",
    "TextBlob is a simpler interface to many of the Natural Language Tool Kit programs.\n",
    "We're using the variable 'blob' to refer to a TextBlob object, which contains our text.\n",
    "It allows us to do useful things like blob.lower() to lowercase all words, or\n",
    "blob.words to access the words as a list.\n",
    "\n",
    "Here we assume your text files are in a directory called 'text_files'.\n",
    "The code below iterates through each filename in the 'text_files' directory,\n",
    "opens it, creates a TextBlob object, lowercases, strips commas, and saves it to a\n",
    "list of texts, here called 'bloblist'. One by one we accumulate a list of prepared texts.\n",
    "'''\n",
    "\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "\n",
    "bloblist = []\n",
    "\n",
    "def check_file(filename):\n",
    "    if not filename.endswith('.txt'):\n",
    "        return None\n",
    "    if filename.startswith('.'):\n",
    "        return None\n",
    "    return filename\n",
    "\n",
    "\n",
    "path = 'text_files/'\n",
    "for filename in os.listdir(path):\n",
    "    if check_file(filename):\n",
    "        with open(os.path.join(path, filename), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            print(filename)\n",
    "            text = f.read()\n",
    "            blob = TextBlob(text)\n",
    "            blob_lower = blob.lower()\n",
    "            blob_strip = blob_lower.strip(',')\n",
    "            bloblist.append(blob_strip)\n",
    "        \n",
    "print(len(bloblist))\n",
    "        \n",
    "# a = '''\n",
    "# In this cell we load a directory of text files into memory using the TextBlob library.\n",
    "# TextBlob is a simpler interface to many of the Natural Language Tool Kit programs.\n",
    "# We're using the variable 'blob' to refer to a TextBlob object, which contains our text.\n",
    "# It allows us to do useful things like blob.lower() to lowercase all words, or\n",
    "# blob.words to access the words as a list.\n",
    "\n",
    "# Here we assume, your text files are in a directory called 'text_files'.\n",
    "# The code below iterates through each filename in the 'text_files' directory,\n",
    "# opens it, creates a TextBlob object, lowercases, strips commas, and saves it to a\n",
    "# list of texts, here called 'bloblist'. One by one we accumulate a list of prepared texts.\n",
    "# '''\n",
    "# blob = TextBlob(a)\n",
    "# print(blob)\n",
    "# print(blob.strip(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of words in each 'blob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 :  1225\n",
      "True\n",
      "False\n",
      "Text 2 :  951\n",
      "True\n",
      "False\n",
      "Text 3 :  1589\n",
      "True\n",
      "False\n",
      "Text 4 :  984\n",
      "True\n",
      "False\n",
      "Text 5 :  1041\n",
      "True\n",
      "False\n",
      "Text 6 :  2255\n",
      "True\n",
      "False\n",
      "Text 7 :  1110\n",
      "True\n",
      "False\n",
      "Text 8 :  460\n",
      "True\n",
      "False\n",
      "Text 9 :  952\n",
      "True\n",
      "False\n",
      "Text 10 :  917\n",
      "True\n",
      "False\n",
      "Text 11 :  756\n",
      "True\n",
      "False\n",
      "Text 12 :  809\n",
      "True\n",
      "False\n",
      "Text 13 :  1237\n",
      "True\n",
      "False\n",
      "Text 14 :  575\n",
      "True\n",
      "False\n",
      "Total: 14861\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code iterates through each document (or 'blob') and uses the len() function to count how many words are in it.\n",
    "Again we use the \"accumulator pattern\" to iteratively add the number of words in each document to a running total\n",
    "variable called corpus_total_words.\n",
    "'''\n",
    "\n",
    "corpus_total_words = 0\n",
    "for i, blob in enumerate(bloblist):\n",
    "    corpus_total_words = corpus_total_words + len(blob.words)\n",
    "    print(\"Text {}\".format(i + 1), \": \", len(blob.words))\n",
    "    print('.'in blob)\n",
    "    print('.' in blob.words)\n",
    "print(\"Total:\", corpus_total_words)\n",
    "\n",
    "\n",
    "# blob.words tao list bao gom cac words, loai tru cac puntuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Most frequent words in document 1\n",
      "\tWord: ('instagram', 30)\n",
      "\tWord: ('snapchat', 21)\n",
      "\tWord: ('stories', 16)\n",
      "\tWord: ('users', 12)\n",
      "\tWord: ('it’s', 10)\n",
      "\tWord: ('people', 8)\n",
      "\tWord: ('social', 8)\n",
      "\tWord: ('facebook', 7)\n",
      "\tWord: ('daily', 7)\n",
      "\tWord: ('”', 7)\n",
      "10 Most frequent words in document 2\n",
      "\tWord: ('information', 9)\n",
      "\tWord: ('humans', 8)\n",
      "\tWord: ('consciousness', 7)\n",
      "\tWord: ('one', 5)\n",
      "\tWord: ('human', 5)\n",
      "\tWord: ('brain', 5)\n",
      "\tWord: ('something', 5)\n",
      "\tWord: ('able', 5)\n",
      "\tWord: ('google', 5)\n",
      "\tWord: ('already', 4)\n",
      "10 Most frequent words in document 3\n",
      "\tWord: ('”', 25)\n",
      "\tWord: ('better', 17)\n",
      "\tWord: ('artificial', 15)\n",
      "\tWord: ('intelligence', 15)\n",
      "\tWord: ('students', 11)\n",
      "\tWord: ('ai', 10)\n",
      "\tWord: ('“what', 10)\n",
      "\tWord: ('computers', 10)\n",
      "\tWord: ('like', 9)\n",
      "\tWord: ('ask', 9)\n",
      "10 Most frequent words in document 4\n",
      "\tWord: ('bluetooth', 48)\n",
      "\tWord: ('devices', 11)\n",
      "\tWord: ('also', 8)\n",
      "\tWord: ('it’s', 8)\n",
      "\tWord: ('5', 8)\n",
      "\tWord: ('things', 7)\n",
      "\tWord: ('wireless', 7)\n",
      "\tWord: ('one', 6)\n",
      "\tWord: ('master', 6)\n",
      "\tWord: ('applications', 5)\n",
      "10 Most frequent words in document 5\n",
      "\tWord: ('work', 11)\n",
      "\tWord: ('interaction', 11)\n",
      "\tWord: ('time', 8)\n",
      "\tWord: ('one', 7)\n",
      "\tWord: ('activities', 7)\n",
      "\tWord: ('people', 6)\n",
      "\tWord: ('local', 6)\n",
      "\tWord: ('system', 6)\n",
      "\tWord: ('networks', 5)\n",
      "\tWord: ('communication', 5)\n",
      "10 Most frequent words in document 6\n",
      "\tWord: ('people', 16)\n",
      "\tWord: ('catholic', 11)\n",
      "\tWord: ('church', 11)\n",
      "\tWord: ('—', 10)\n",
      "\tWord: ('one', 8)\n",
      "\tWord: ('get', 8)\n",
      "\tWord: ('that’s', 8)\n",
      "\tWord: ('sex', 7)\n",
      "\tWord: ('religion', 6)\n",
      "\tWord: ('spent', 6)\n",
      "10 Most frequent words in document 7\n",
      "\tWord: ('know', 22)\n",
      "\tWord: ('us', 9)\n",
      "\tWord: ('information', 9)\n",
      "\tWord: ('children', 7)\n",
      "\tWord: ('it’s', 6)\n",
      "\tWord: ('world', 6)\n",
      "\tWord: ('night', 5)\n",
      "\tWord: ('data', 5)\n",
      "\tWord: ('every', 5)\n",
      "\tWord: ('pm', 4)\n",
      "10 Most frequent words in document 8\n",
      "\tWord: ('tunnels', 6)\n",
      "\tWord: ('consider', 5)\n",
      "\tWord: ('musk', 4)\n",
      "\tWord: ('public', 4)\n",
      "\tWord: ('technology', 4)\n",
      "\tWord: ('company', 3)\n",
      "\tWord: ('companies', 3)\n",
      "\tWord: ('rights', 3)\n",
      "\tWord: ('must', 3)\n",
      "\tWord: ('private', 3)\n",
      "10 Most frequent words in document 9\n",
      "\tWord: ('ai', 11)\n",
      "\tWord: ('platform', 6)\n",
      "\tWord: ('experience', 5)\n",
      "\tWord: ('—', 5)\n",
      "\tWord: ('chupa', 4)\n",
      "\tWord: ('intelligence', 4)\n",
      "\tWord: ('way', 4)\n",
      "\tWord: ('new', 4)\n",
      "\tWord: ('customers', 4)\n",
      "\tWord: ('customer', 4)\n",
      "10 Most frequent words in document 10\n",
      "\tWord: ('technology', 9)\n",
      "\tWord: ('like', 7)\n",
      "\tWord: ('cities', 7)\n",
      "\tWord: ('companies', 7)\n",
      "\tWord: ('work', 7)\n",
      "\tWord: ('new', 6)\n",
      "\tWord: ('beauty', 6)\n",
      "\tWord: ('world', 5)\n",
      "\tWord: ('energy', 5)\n",
      "\tWord: ('data', 5)\n",
      "10 Most frequent words in document 11\n",
      "\tWord: ('robot', 10)\n",
      "\tWord: ('design', 9)\n",
      "\tWord: ('anthropology', 8)\n",
      "\tWord: ('user', 7)\n",
      "\tWord: ('anthropological', 6)\n",
      "\tWord: ('product', 6)\n",
      "\tWord: ('people', 6)\n",
      "\tWord: ('methods', 5)\n",
      "\tWord: ('technology', 5)\n",
      "\tWord: ('might', 5)\n",
      "10 Most frequent words in document 12\n",
      "\tWord: ('chatbots', 24)\n",
      "\tWord: ('chatbot', 16)\n",
      "\tWord: ('artificial', 8)\n",
      "\tWord: ('intelligence', 8)\n",
      "\tWord: ('credit', 5)\n",
      "\tWord: ('information', 5)\n",
      "\tWord: ('businesses', 5)\n",
      "\tWord: ('humans', 4)\n",
      "\tWord: ('experience', 4)\n",
      "\tWord: ('facebook', 4)\n",
      "10 Most frequent words in document 13\n",
      "\tWord: ('—', 12)\n",
      "\tWord: ('mini', 10)\n",
      "\tWord: ('nintendo', 9)\n",
      "\tWord: ('like', 9)\n",
      "\tWord: ('year', 7)\n",
      "\tWord: ('disney', 7)\n",
      "\tWord: ('nes', 6)\n",
      "\tWord: ('know', 5)\n",
      "\tWord: ('seem', 5)\n",
      "\tWord: ('doesn’t', 5)\n",
      "10 Most frequent words in document 14\n",
      "\tWord: ('phone', 9)\n",
      "\tWord: ('pin', 7)\n",
      "\tWord: ('information', 7)\n",
      "\tWord: ('given', 5)\n",
      "\tWord: ('machine', 4)\n",
      "\tWord: ('learning', 4)\n",
      "\tWord: ('guess', 4)\n",
      "\tWord: ('four-digit', 4)\n",
      "\tWord: ('use', 4)\n",
      "\tWord: ('hack', 4)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Notice the accumulator pattern is used often. We loop through our data (bloblist), creating a new data\n",
    "structure for each text (blob_frequency_dictionary). We then loop through the words of each text (blob.words)\n",
    "and find the frequency of that word in the text - ie how many times it occurs in that text - and record it. \n",
    "This blob_frequency_dictionary data looks something like: {'the': 38, 'to':33, 'and':28 ...}\n",
    "By default no stopwords are applied, but you can uncomment a line below to apply the NLTK English stopword list.\n",
    "This will filter out function words, leaving you with the 'content' or 'lexical' words that are likely to be\n",
    "of interest for this assignment.\n",
    "After this we sort by the \n",
    "'''\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    blob_frequency_dictionary = {}\n",
    "    for word in blob.words:\n",
    "        count = blob.words.count(word)\n",
    "        if word not in blob_frequency_dictionary:\n",
    "            # add stopword filtering here by uncommenting the next line, and indenting the line after it\n",
    "            if word.lower() not in sw:\n",
    "                blob_frequency_dictionary[word] = count\n",
    "        sorted_words = sorted(blob_frequency_dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "    print('10 Most frequent words in document {}'.format(i + 1))\n",
    "    for word in sorted_words[:10]:\n",
    "        print(\"\\tWord:\", word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some NLTK text objects and view concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Text: lots of people are saying that instagram just...>,\n",
       " <Text: in the future philosophers will be gods there...>,\n",
       " <Text: how to teach ai to kids science fiction...>,\n",
       " <Text: 6 things you didn’t know about bluetooth like...>,\n",
       " <Text: neural networks as the architecture of human work...>,\n",
       " <Text: the big business of organized religion or why...>,\n",
       " <Text: the power of not knowing by tobias van...>,\n",
       " <Text: elon musk’s boring company tunnels might be a...>,\n",
       " <Text: what mega chupa chups taught me about artificial...>,\n",
       " <Text: while i’m a big fan of cool new...>,\n",
       " <Text: should robots dance how anthropological methods can answer...>,\n",
       " <Text: chatbots as loyal friends to humans age of...>,\n",
       " <Text: nintendo’s surface first published on 4/29/17 on 5ish...>,\n",
       " <Text: machine learning just made it really easy to...>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Note your texts may not be in the order they appear as files, but you can view the list\n",
    "by running this cell.\n",
    "'''\n",
    "\n",
    "from nltk.text import Text\n",
    "\n",
    "nltk_text_list = []\n",
    "\n",
    "for blob in bloblist:\n",
    "    nltk_text = Text(blob.words)\n",
    "    nltk_text_list.append(nltk_text)\n",
    "\n",
    "nltk_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "erent ways people use these platforms to connect with others snapchat is not just another\n",
      "Displaying 2 of 2 matches:\n",
      "ting the two apps as similar platforms to begin with is a fruitless exercise first it’s w\n",
      "le would have ever joined the platform to begin with “instagram is still a place for user\n",
      "Displaying 8 of 8 matches:\n",
      "                                          people are saying that instagram just officially\n",
      "ached 200 million daily active users daus people were quick to pounce on that significant \n",
      "g to be the same.” instagram stories many people say that instagram stories offer a more e\n",
      "a more enjoyable experience because “more people see your story.” but they are thinking of\n",
      "s allowing users to discover content from people they don’t follow but the snaps that are \n",
      "pchat users it’s unclear if many of these people would have ever joined the platform to be\n",
      "ather than focusing on irrelevant metrics people should think critically about the differe\n",
      "think critically about the different ways people use these platforms to connect with other\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "You can select a text by changing the number 6 below - in this example 0-13 will work.\n",
    "You can also specify how many lines and how 'wide' you'd like the text snippet to be.\n",
    "'''\n",
    "\n",
    "nltk_text_list[0].concordance('connect', lines=50, width=90)\n",
    "nltk_text_list[0].concordance('begin', lines=50, width=90)\n",
    "\n",
    "nltk_text_list[0].concordance('people', lines=50, width=90)\n",
    "# lines: The number of lines to display (default=25)\n",
    "# width: the number of characters displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use NLTK's similar() and common_contexts() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "NLTK's similar() method finds words that appear in the same context, ie have the same words on either side.\n",
    "'''\n",
    "\n",
    "nltk_text_list[0].similar('connect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_with\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "NLTK's common_contexts() methods works in the opposite direction to similar().\n",
    "Give it two words that share a context (ie matching words on either side), and\n",
    "it will show you what the context words are. \n",
    "There can be multiple contexts, and this gets more interesting the larger our \n",
    "texts are.\n",
    "'''\n",
    "\n",
    "nltk_text_list[0].common_contexts(['connect', 'begin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency - Inverse Document Frequency (TF-IDF) is a weighting that finds words that are characteristic of a document within a corpus. It finds words that appear quite frequently in a given document, but not in the other documents. \n",
    "\n",
    "Words that occur only once or twice in a single document and not in any other documents don't tell us a lot about the document - they may be just the whim of the writer. Similarly, words that appear a lot in all the documents don't tell us much about the differences between documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions\n",
    "\n",
    "For each word in the corpus:\n",
    "\n",
    "**Term Frequency** (tf) = frequency of the word in each document\n",
    "\n",
    "**Document Frequency** (df) = number of documents in the corpus containing the word\n",
    "\n",
    "**Inverse Document Frequency** (idf) = (logarithm of) the number of documents divided by the document frequency for the word\n",
    "\n",
    "So tf-idf for a word in the corpus is calculated by tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c7299b07-09e1-40dd-8db2-7ea852fb321e"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Here are function definitions for tf, df (here called 'n_containing), idf and tfidf.\n",
    "'''\n",
    "\n",
    "import math\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "88a7c217-fb86-4a0f-b49d-d0610b5d0e4c"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tinstagram, TF-IDF: 0.04765\n",
      "\tsnapchat, TF-IDF: 0.02148\n",
      "\tstories, TF-IDF: 0.01636\n",
      "\tusers, TF-IDF: 0.0083\n",
      "\tsocial, TF-IDF: 0.00818\n",
      "\tplatform, TF-IDF: 0.00755\n",
      "\tfacebook, TF-IDF: 0.00716\n",
      "\tdaily, TF-IDF: 0.00716\n",
      "\tapp, TF-IDF: 0.00716\n",
      "\tsnap, TF-IDF: 0.00635\n",
      "Top words in document 2\n",
      "\tconsciousness, TF-IDF: 0.00922\n",
      "\tlikely, TF-IDF: 0.00648\n",
      "\texperts, TF-IDF: 0.00614\n",
      "\tcomplexity, TF-IDF: 0.00614\n",
      "\tenhance, TF-IDF: 0.00614\n",
      "\tthought, TF-IDF: 0.00614\n",
      "\tfrictionless, TF-IDF: 0.00614\n",
      "\tinstant, TF-IDF: 0.00614\n",
      "\tfriction, TF-IDF: 0.00614\n",
      "\tcollective, TF-IDF: 0.00614\n",
      "Top words in document 3\n",
      "\tcomputers, TF-IDF: 0.01225\n",
      "\tstudents, TF-IDF: 0.01066\n",
      "\t“what, TF-IDF: 0.00788\n",
      "\tvideos, TF-IDF: 0.00735\n",
      "\task, TF-IDF: 0.0071\n",
      "\t”, TF-IDF: 0.00695\n",
      "\tartificial, TF-IDF: 0.00654\n",
      "\tintelligence, TF-IDF: 0.00654\n",
      "\tcuriosity, TF-IDF: 0.00612\n",
      "\tflight, TF-IDF: 0.00612\n",
      "Top words in document 4\n",
      "\tbluetooth, TF-IDF: 0.09492\n",
      "\twireless, TF-IDF: 0.01384\n",
      "\tdevices, TF-IDF: 0.01151\n",
      "\t5, TF-IDF: 0.01019\n",
      "\tstandard, TF-IDF: 0.00989\n",
      "\twi-fi, TF-IDF: 0.00989\n",
      "\tmaster, TF-IDF: 0.00939\n",
      "\tviking, TF-IDF: 0.00791\n",
      "\tharald, TF-IDF: 0.00791\n",
      "\tslave, TF-IDF: 0.00791\n",
      "Top words in document 5\n",
      "\tinteraction, TF-IDF: 0.01628\n",
      "\twork, TF-IDF: 0.01324\n",
      "\tactivities, TF-IDF: 0.01036\n",
      "\torganizational, TF-IDF: 0.00935\n",
      "\tlocal, TF-IDF: 0.00888\n",
      "\tcontributions, TF-IDF: 0.00748\n",
      "\ttop-down, TF-IDF: 0.00748\n",
      "\tnetworks, TF-IDF: 0.0074\n",
      "\tsystem, TF-IDF: 0.00722\n",
      "\tcommunication, TF-IDF: 0.00602\n",
      "Top words in document 6\n",
      "\tcatholic, TF-IDF: 0.00949\n",
      "\tchurch, TF-IDF: 0.00949\n",
      "\tmy, TF-IDF: 0.00902\n",
      "\the, TF-IDF: 0.00667\n",
      "\treligion, TF-IDF: 0.00518\n",
      "\tsex, TF-IDF: 0.00478\n",
      "\torganized, TF-IDF: 0.00431\n",
      "\tteapot, TF-IDF: 0.00431\n",
      "\tmarriage, TF-IDF: 0.00431\n",
      "\tspent, TF-IDF: 0.0041\n",
      "Top words in document 7\n",
      "\tchildren, TF-IDF: 0.00971\n",
      "\tnight, TF-IDF: 0.00877\n",
      "\tpm, TF-IDF: 0.00701\n",
      "\tour, TF-IDF: 0.00697\n",
      "\twatches, TF-IDF: 0.00526\n",
      "\tbacon, TF-IDF: 0.00526\n",
      "\trate, TF-IDF: 0.00526\n",
      "\theads, TF-IDF: 0.00526\n",
      "\tknow, TF-IDF: 0.00478\n",
      "\tinformation, TF-IDF: 0.00454\n",
      "Top words in document 8\n",
      "\ttunnels, TF-IDF: 0.02538\n",
      "\tmusk, TF-IDF: 0.0134\n",
      "\tconsider, TF-IDF: 0.01119\n",
      "\trights, TF-IDF: 0.01005\n",
      "\tprivate, TF-IDF: 0.01005\n",
      "\tboring, TF-IDF: 0.00846\n",
      "\ttesla, TF-IDF: 0.00846\n",
      "\tdig, TF-IDF: 0.00846\n",
      "\tbeneath, TF-IDF: 0.00846\n",
      "\tground, TF-IDF: 0.00846\n",
      "Top words in document 9\n",
      "\tplatform, TF-IDF: 0.00971\n",
      "\tchupa, TF-IDF: 0.00818\n",
      "\tai, TF-IDF: 0.00801\n",
      "\tmarketers, TF-IDF: 0.00647\n",
      "\tmega, TF-IDF: 0.00613\n",
      "\tchups, TF-IDF: 0.00613\n",
      "\tmall, TF-IDF: 0.00613\n",
      "\tregister, TF-IDF: 0.00613\n",
      "\t“yes, TF-IDF: 0.00613\n",
      "\treal.”, TF-IDF: 0.00613\n",
      "Top words in document 10\n",
      "\tcities, TF-IDF: 0.01485\n",
      "\tbeauty, TF-IDF: 0.01273\n",
      "\twork, TF-IDF: 0.00956\n",
      "\tenergy, TF-IDF: 0.00683\n",
      "\tcompanies, TF-IDF: 0.00647\n",
      "\tcity, TF-IDF: 0.00637\n",
      "\ttech, TF-IDF: 0.00504\n",
      "\ttrend, TF-IDF: 0.00504\n",
      "\tcountries, TF-IDF: 0.00504\n",
      "\tproducts, TF-IDF: 0.00462\n",
      "Top words in document 11\n",
      "\tanthropology, TF-IDF: 0.02059\n",
      "\tanthropological, TF-IDF: 0.01544\n",
      "\trobot, TF-IDF: 0.01362\n",
      "\tsavioke, TF-IDF: 0.01287\n",
      "\tdesign, TF-IDF: 0.01226\n",
      "\tdelivery, TF-IDF: 0.0103\n",
      "\tmethods, TF-IDF: 0.01019\n",
      "\tproduct, TF-IDF: 0.00994\n",
      "\tuser, TF-IDF: 0.00953\n",
      "\tdevelopment, TF-IDF: 0.00815\n",
      "Top words in document 12\n",
      "\tchatbots, TF-IDF: 0.0457\n",
      "\tchatbot, TF-IDF: 0.03047\n",
      "\tcredit, TF-IDF: 0.01203\n",
      "\tbusinesses, TF-IDF: 0.01203\n",
      "\tonline, TF-IDF: 0.00722\n",
      "\tartificial, TF-IDF: 0.00685\n",
      "\tintelligence, TF-IDF: 0.00685\n",
      "\tfacebook, TF-IDF: 0.00619\n",
      "\tmessenger, TF-IDF: 0.00619\n",
      "\tinteract, TF-IDF: 0.00571\n",
      "Top words in document 13\n",
      "\tmini, TF-IDF: 0.01573\n",
      "\tnintendo, TF-IDF: 0.01416\n",
      "\tdisney, TF-IDF: 0.01101\n",
      "\tnes, TF-IDF: 0.00944\n",
      "\tpark, TF-IDF: 0.00629\n",
      "\tsnes, TF-IDF: 0.00629\n",
      "\t—, TF-IDF: 0.00543\n",
      "\tswitch, TF-IDF: 0.00498\n",
      "\tquarter, TF-IDF: 0.00472\n",
      "\tlaunch, TF-IDF: 0.00472\n",
      "Top words in document 14\n",
      "\tpin, TF-IDF: 0.02369\n",
      "\tphone, TF-IDF: 0.01961\n",
      "\tguess, TF-IDF: 0.01354\n",
      "\tfour-digit, TF-IDF: 0.01354\n",
      "\thack, TF-IDF: 0.01354\n",
      "\tcollect, TF-IDF: 0.01354\n",
      "\tgiven, TF-IDF: 0.01089\n",
      "\tiot, TF-IDF: 0.01072\n",
      "\taccuracy, TF-IDF: 0.01015\n",
      "\torientation, TF-IDF: 0.01015\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here we loop through the list of documents called 'bloblist'.\n",
    "Scores is a dictionary of key:value pairs. \n",
    "Each key is a word in the document and the value is its tfidf score. \n",
    "Results are sorted by the tfidf score with the largest value at the top.\n",
    "Lastly we print the first 10 results for each document.\n",
    "'''\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:10]:\n",
    "        print(\"\\t{}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun_phrases in document 1\n",
      "========================================\n",
      "['100m instagram followers', '400m daus', 'achievement — it’s', 'active user', 'active users', 'active users', 'active users', 'active users”', 'advertisers’ “experimental bucket”', 'average snapchatter', 'average user', 'average video view times', 'beyonce blasts', 'broadcast model', 'broadcast platform', 'camera company', 'checks instagram stories', 'communication platform', 'company’s flagship app', 'consumer behavior', 'craven senior vice president', 'daily active users', 'daily basis.” instagram stories facebook won’t', 'dead fam', 'different set', 'different ways people', 'doesn’t stoke', 'early stage investor', 'enjoyable experience', 'equate apples', 'equate snapchat', 'essential destination', 'facebook messenger', 'facebook messenger', 'fertile niche', 'follower counts', 'high level', 'imaginary war', 'important part', 'instagram stories', 'instagram stories', 'instagram stories', 'instagram stories', 'instagram stories', 'instagram stories', 'instagram stories', 'instagram stories', 'instagram stories’ 200m daus', 'instagram story', 'instagram story', 'instagram vs snapchat', 'instagram’s total', 'intentional communication', 'internal discovery mechanism', 'irrelevant metrics', 'it’s unclear', 'it’s worth', 'likely end', 'mark zuckerberg', 'mass exposure', 'new norms', 'noah mallin head', 'one-to-many communication', 'over.” “i', 'own app', 'premium content', 'productive discussion', 'public stories', 'reason it’s', 'same.” instagram stories', 'search function', 'significant number', 'silicon valley', 'similar platforms', 'smartphone photography', 'snapchat doesn’t', 'snapchat is…', 'snapchat tech observers', 'snapchat user', 'snapchat video', 'snapchat’ here’s', 'snapchat’s signature', 'snapchat’s total 161m', 'social media —', 'social network', 'social networks', 'social networks it’s', 'social platforms', 'social products aren’t', 'tech titans', 'tv dollars', 'user accounts', 'user base engages', 'user behaviors', 'video consumption', 'would-be snapchat users', '‘killed’ snapchat', '‘snapchat effect.’', '“more people', '“rip snap', '“social media company.” instagram', '“your mom', '” gemma', '” parker thompson', '” “instagram', '” “is snapchat', '” “snapchat']\n",
      "========================================\n",
      "Noun_phrases in document 2\n",
      "========================================\n",
      "['account moore’s law', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'available evidence', 'available information', 'biological humans', 'brain implants', 'brain stores information', 'collective consciousness', 'collective consciousness', 'collective knowledge', 'compile information', 'consciousness doesn’t', 'couldn’t enhance', 'digestible form', 'direct link', 'download information', 'elon musk’s neuralink', 'entire concept', 'evolutionary position', 'financial transactions', 'format compatible', 'foundational debates', 'friction scale', 'frictionless data acquisition', 'frictionless data acquisition', 'frictionless data creation', 'future philosophers', 'human brain', 'human brains', 'human brains', 'human knowledge', 'human level', 'instant download', 'instant download', 'instant upload', 'instantaneous google search', 'instantaneous knowledge', 'large degree', 'likely explanation', 'long time', 'new things', 'new thoughts', 'physical laws', 'powerful individuals', 'prominent players', 'public perception', 'rare diseases', 'relevant information', 'replicate biological storage methods', 'static thing', 'such heights', 'true god', 'understandable format', 'young adult’s lifetime']\n",
      "========================================\n",
      "Noun_phrases in document 3\n",
      "========================================\n",
      "['.” lead', '1. motivations', '3. ideas', 'actionable understanding', 'ai introduction', 'american science fiction drama film', 'andrew hodges', 'artificial flight', 'artificial flight', 'artificial flight', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'bbc drama', 'biography alan', 'bit tricky', 'british government', 'childlike android', 'class time', 'classroom discussion', 'column b', 'column b.', 'communication… humans', 'computer science', 'country i’m', 'crazy idea', 'cultivate curiosity', 'decent surface level answers', 'different countries', 'don’t need', 'everyday devices', 'expressive ai computational creativity procedural content generation computer vision robotics', 'fancy stanford summer course', 'followup questions', 'future scenarios', 'futuristic post-climate', 'game time', 'german intelligence codes', 'good trailer', 'graduate student level', 'high schools', 'historical context', 'historical drama thriller film', 'historical motivations', 'human attributes', 'human movement', 'human-made things', 'i end', 'i lead', 'i show', 'ideas worth', 'imitation game', 'imitation game', 'imitation game', 'important concepts', 'india versus peru', 'instructor loads', 'interesting games', 'kids science fiction', 'lego sudoku bot', 'machine learning', 'machine learning', 'machine learning', 'math teacher', 'motivate invention', 'movie a.i', 'movie makers', 'movie trailers', 'natural language generation', 'new experience', 'nordic seagulls', 'novel pursuits', 'pac-man collides', 'pac-man ghosts', 'pancake bot', 'paper airplane', 'paper ball', 'pooping… … ok', 'power cables', 'productive curiosity', 'real-life british cryptanalyst alan', 'right direction', 'right kind', 'right questions', 'robot arm', 'robot dog', 'screen time', 'sleeping.” i', 'stage play', 'stars benedict cumberbatch', 'steven spielberg', 'students i’ve', 'ultimate motivation', 'various parts', 'video 2. curiosity', 'ways i', 'world war', 'young students', 'youtube videos', 'youtube videos', 'youtube videos i use', '“artificial intelligence.”', '“grown-up” parts', '“self-driving” relies', '“smart machines', '“the how.” show', '“the what.”', '“the why.”', '“your teacher', '” i', '” “do birds', '” “how', '” “how', '” “if i', '” “what', '” “what', '” “what', '” “what', '” “why']\n",
      "========================================\n",
      "Noun_phrases in document 4\n",
      "========================================\n",
      "['# askiot', 'bluetooth connections', 'bluetooth device', 'bluetooth devices', 'bluetooth headsets', 'bluetooth logo', 'bluetooth radio', 'bluetooth radio chip', 'bluetooth sig didn’t', 'bluetooth sig’s announcement', 'bluetooth uses slavery', 'bluetooth uses weak signals', 'bluetooth version', 'boasts quadruple', 'boosts broadcast', 'broad variety', 'concepts you’d', 'device runs', 'different piconet', 'door locks', 'easy set-up', 'embrace technological advancements', 'enable wireless headsets', 'exciting development', 'frans g. bengtsson', 'harald bluetooth', 'harald bluetooth’s initials', 'harald “blåtand”', 'harald “blåtand” gormsson', 'hvac system', 'individual devices', 'industrial applications', 'industrial applications', 'industrial internet', 'interesting things', 'iot concepts', 'iot questions', 'iot.” —bluetooth', 'it’s usage', 'jim kardach', 'jim kardach', 'long ships”', 'low range', 'master transmits information', 'master-slave relationship', 'master-slave relationship', 'mesh network', 'minimal configuration', 'multiple devices', 'necessary signal', 'new benefits', 'new hardware', 'new medium posts', 'noisy environments', 'non-technical terms', 'people experience', 'portable equipment', 'previous versions', 'radio frequency', 'reading “the', 'separate purposes', 'short bursts', 'short question you’d', 'similar way', 'slave listens', 'smart home', 'smart home don’t need high bandwidth connections', 'smart home setting', 'smart homes', 'software component', 'software determines what’s', 'special interest group', 'special interest group', 'standard wireless technology standards', 'that’s why…', 'tiny computer chip', 'video game controllers', 'watch batter', 'week we’ll', 'wi-fi poses', 'wireless communications protocols', 'wireless keyboard', 'wireless speaker', 'wireless technology', 'wireless technology', '“noisy” environments', '“with bluetooth']\n",
      "========================================\n",
      "Noun_phrases in document 5\n",
      "========================================\n",
      "['//avc.com/2017/04/decentralized-self-organizing-systems/ serverless architectures', 'adrian cockroft', 'appropriate system', 'architecture refers', 'artificial intelligence', 'artificial intelligence', 'aws lambda https', 'black box', 'bottom-up manner', 'capital investment', 'complex networks', 'complex system', 'contextual interdependence', 'contextual need', 'contribute pieces', 'creative services', 'daily organizational life', 'decade ago.”', 'decisive factors', 'different communications environment', 'different parts', 'digital world', 'doug griffin', 'dual process', 'efficient method', 'high cost', 'high-quality interaction', 'higher-value activities', 'homogeneous resources', 'human work', 'human work', 'important metric', 'independent choices', 'large number', 'local events', 'local interaction', 'local interaction', 'longer-term relationship', 'low-quality interaction', 'machine learning', 'machine learning', 'machine learning', 'machine learning work', 'main tasks', 'mainstream management paradigm', 'natural formation', 'neural networks', 'neural networks', 'neural networks facilitate', 'neuron links', 'new architecture', 'new event', 'new value creation architectures', 'open field', 'organizational design', 'organizational forms', 'organizational structure', 'organizational system', 'own principles', 'own rules', 'particular event', 'peer interaction', 'person interacts', 'post-blockchain smart contracts', 'powerful agents', 'present ways', 'problem definition', 'production logic', 'resource allocation', 'resource allocation point', 'server links', 'short-term communication', 'systemic advantages', 'systems http', 'temporal interaction', 'top-down process', 'top-down system', 'traditional competitive selection', 'traditional production hierarchies', 'transparent environment', 'valuable contributions', 'value creation', 'way enterprise resource planning', 'whole behaves', 'whole network', 'yochai benkler', '“the cost']\n",
      "========================================\n",
      "Noun_phrases in document 6\n",
      "========================================\n",
      "['$ 1.5m', '$ 5.2m', 'age i don’t', 'ancient institution', 'ancient institutions', 'anti-marriage equality campaigns', 'aren’t catholic', 'bad guy', 'bernie sanders', 'big business', 'bishops’ conferences', 'brick wall', 'bulletproof popemobile', 'bullshit detection', 'bullshit detection', 'can’t rationalize', 'catholic church', 'catholic church', 'catholic church runs', 'catholic church’s attempts', 'catholic hospitals', 'catholic hospitals', 'catholic ideology', 'catholic people won’t', 'catholic school', 'character’s possessions', 'child sex abuse cases', 'childhood summers', 'church money', 'claim license', 'comment — that’s', 'consistent basis', 'corrupt institution trumps', 'countless atrocities', 'countless hours', 'countless hours', 'countless new sex abuse cases', 'criminal case', 'crucial distinction', 'crucial part', 'dad’s advice', 'day fifth-grade', 'dead dogma', 'dead dogma', 'different hospital', 'doubt it’s', 'eastern philosophies', 'egregious case', 'elementary school career', 'emergency surgery', 'exceptional life', 'existential uncertainty', 'exorbitant sums', 'family issues', 'faulty teapot logic', 'gay-marriage debate', 'gold coin', 'good guy', 'grade self', 'great injustice', 'happy accident', 'hard way', 'health care', 'high school', 'high school i’ve', 'hospital beds', 'human experience', 'human purposes', 'hypocrite.” i', 'i can’t', 'i don’t', 'individual cases', 'institution run', 'institutions prey', 'isis suicide bombers', 'it’s brutal', 'it’s worth', 'judeo-christian ethic', 'lgbt kids', 'lgbt people', 'lie i’d', 'limitations reform efforts', 'logic applies', 'longstanding personal grudge', 'marie collins', 'means they’re', 'medical care', 'men —', 'mess he’s', 'miscarriages —', 'miscellaneous expenses', 'mmorpg runescape', 'molest kids', 'moral code', 'moral code” idea', 'moral ideology', 'moral superiority', 'mormons won’t', 'muslim refugees', 'mysterious character', 'national operation budget', 'natural human response', 'new jersey', 'opposition efforts', 'own business', 'own destiny', 'own stupid self', 'particular heavyweight', 'people buying', 'person’s perspective', 'pope francis', 'psychological composition', 'public relations fallout', 'question –', 'questionable assignment', 'real crux', 'regular basis', 'religion teacher', 'religious institutions', 'revolution form', 'rough breakdown', 'runescape account', 'same-sex marriage', 'same-sex marriage', 'same-sex marriage', 'selfless human', 'seminal moment', 'sex abuse', 'sex abuse victim', 'sex abuse victim', 'similar bullshit', 'similar legislative proposals', 'skill level', 'skill levels', 'skills i', 'slow pace', 'someone’s skill level', 'spaghetti monster', 'special underwear', 'state capitols', 'state legislator', 'states.” —', 'study scripture', 'subsidiary money', 'subsidiary organizations', 'tax-exempt money', 'teapot argument —', 'teapot whatsoever', 'temporary civil windows', 'that’s way', 'time period', 'top lobbyists', 'ultimate threshold', 'valuable life lesson', 'various capacities', 'vatican curia', 'virgins they’ll encounter', 'worthy investment', '— religion', '“although i respect', '“self interest']\n",
      "========================================\n",
      "Noun_phrases in document 7\n",
      "========================================\n",
      "['apparent effect', 'apple watches', 'brains process', 'calories you’ve', 'can’t hurt', 'cave drawings', 'circle people', 'civilization age', 'combustion engines', 'crave information', 'ctrl alt design algorithms', 'current events', 'current location', 'data lets', 'dave eggers', 'deep sleep', 'ever-present daylight', 'ever-present daylight sounds', 'facetime calls', 'fancy watches', 'favorite comic artists', 'francis bacon', 'friend ate', 'friends app', 'google can’t control', 'habitable place', 'heart rate', 'heart rate', 'helene eksterowicz', 'horror movie', 'i beg', 'information overload', 'lessen anxiety', 'means “knowledge', 'melon heads', 'mr. bacon', 'other’s locations', 'own kids', 'parents’ tv screens', 'personal blog it’s', 'personal data', 'physical capabilities', 'political angst', 'poorlydrawnlines.com —', 'potential criminal', 'public service announcement', 'riotous time', 'scientia potentia est', 'search engines', 'security cameras', 'sex life', 'shallow interactions', 'short circuit', 'snapchat stories', 'souls need', 'steps you’ve', 'strap devices', 'tobias van schneider', 'toothpaste preference', 'tv screens', 'wasn’t francis bacon', 'wikipedia page', '“stay informed”', '“the bachelor', '“the circle', '” “we']\n",
      "========================================\n",
      "Noun_phrases in document 8\n",
      "========================================\n",
      "['active oil', 'clean water', 'climate event', 'company tunnels', 'curious relationship', 'electric cars', 'elon musk’s', 'eminent domain', 'foundation displacement', 'human survival', 'humanity elon musk', 'humble pursuits', 'i don’t claim', 'i don’t claim', 'ice age', 'immediate application', 'incredible consequences', 'incredible potential', \"individual 's location\", 'los angeles', 'military purposes', 'musk companies', 'place musk', 'plan b', 'potential alternative uses', 'private land', 'private profits', 'property owners', 'property rights', 'public land', 'public resource', 'radical technology company', 'risk subsidence', 'smart people', 'solar panels', 'space travel', 'spacex technology', 'such tunnels', 'supports structures', 'tax dollars']\n",
      "========================================\n",
      "Noun_phrases in document 9\n",
      "========================================\n",
      "['1/customer service — ai start-up soul machines', '2/retail engagement — mall', '4/virtual assistant communication —', '6,000+ bots', 'age ] covergirl', 'ai application', 'ai comfort level', 'ai interface', 'ai systems', 'amazon echo', 'america merrill lynch', 'andrew blair', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence ai', 'australian government', 'bot experiences', 'broadcast tv', 'certain actions', 'certain situations', 'channel management— kik', 'chatbot —', 'chupa chup', 'common questions', 'computer engagement', 'computer intelligence applications', 'custom experience', 'customer expectation', 'customer interactions', 'customer service assistant', 'different platforms', 'different rules', 'direct messages', 'due time', 'embrace what’s', 'engagement platform', 'experience list formulator', 'facebook aren’t', 'facebook messenger', 'favorite laundry detergent', 'giant lollipops', 'giant lollipops', 'global market', 'good tip', 'google home', 'guests plan', 'human beings', 'human intervention', 'ibm watson', 'influencer kalani hilliker', 'magic trick', 'mark zuckerberg', 'mega chupa chups', 'mega chupa chups', 'mega chupa chups', 'messenger api', 'modern media culture', 'new employee', 'new way', 'omnichannel bot', 'omnichannel experience', 'potential people', 'previous customer', 'real-time response', 'real.” customers', 'realistic facial expressions', 'recent chatbot', 'samsung’s bixby', 'search results', 'select gift items', 'similar patterns', 'simple sign', 'smart homes', 'soul machines’ aim', 'time period', 'two-time academy award winner mark sagar', 'users worldwide', 'virtual assistants', 'wacky t-shirts', 'watson’s conversational api', 'yes i', 'you’re building', '“the world’s', '” companies']\n",
      "========================================\n",
      "Noun_phrases in document 10\n",
      "========================================\n",
      "['//newscenter.lbl.gov/2016/08/24/median-installed-price-solar-united-states-fell-5-12-2015/ nootropics', '500k villages', '@ seyifabo', 'address systems', 'ambitious digital country plan', 'aren’t part', 'artificial intelligence', 'battery technology', 'beauty l’oreal generates $ 28bn revenue', 'beauty products', 'beauty products', 'beauty products', 'beauty tech trend', 'big fan', 'blue origin', 'business sense', 'city data', 'city movement', 'city scale', 'civic tech', 'clean energy etc', 'collaboration technology', 'cool new technology', 'cool technology', 'core work', 'countrywide id initiative', 'critical projects', 'deep collaborative work', 'digital countries', 'electricity prices', 'elon musk', 'energy services companies', 'fantastic job', 'full steam', 'future work', 'geek fest', 'google’s sidewalk labs', 'grid defection', 'grid structure', 'grid-scale solar', 'healthy energy', 'hour commute', 'i won’t', 'individual needs', 'large corporations', 'large systems', 'local grocery store', 'logical step', 'l’oréal’s technology incubator', 'mapo mask', 'mark zuckerberg', 'money they’ve', 'new containers', 'new revenue stream', 'new technology', 'new trend', 'new type', 'particular commute', 'plate number', 'priscilla chan', 'productivity video', 'protein lunches', 'renewable sources', 'rotisserie chicken', 'savvy brands', 'service economy transition', 'slack etc', 'smart cities', 'smart hairbrush', 'space exploration company', 'spacex customers', 'system-scale tech jeff bezos’', 'tad bit', 'target ads', 'technology companies', 'temporary employment', 'time zones', 'traffic data', 'true diversity', 'utility partner', 'way home', 'weather data', 'whole countries', 'willing companies', 'work structure', '‘better brains', '‘regular employee’', '‘the inevitable’']\n",
      "========================================\n",
      "Noun_phrases in document 11\n",
      "========================================\n",
      "['anthropological challenge', 'anthropological methods', 'anthropological methods', 'anthropological methods', 'anthropological methods', 'anthropological methods', 'anthropology enlightens companies', 'autonomous robot', 'case studies', 'comfortable sportswear', 'company desires', 'company founders', 'company’s challenge', 'cultural barriers', 'customer happiness', 'customer satisfaction', 'decision making', 'delivery cycles', 'delivery robot', 'development team', 'didn’t invest', 'different skill', 'early 1990s', 'end user', 'everyday lives', 'express happiness', 'favorite case study', 'futurists dream', 'genevieve bell', 'good market', 'google ventures', 'helleka koppel', 'hotel business', 'hotel customers', 'hotel room doors', 'important strategic tools', 'intel’s data center group', 'interaction design master students’ anthropology course', 'interaction design master students’ anthropology course', 'maarja mõtus', 'marko ubu', 'mexican food market', 'muslim women', 'new markets', 'new product', 'office use', 'old interaction design research technique', 'one-week design sprint', 'one-week design sprint', 'oz experiment method', 'peak hours', 'people smile', 'persistent problem', 'physical product design', 'product development', 'product development', 'product development process”', 'proekspert designers taavi aher', 'proekspert’s design team', 'real hotel environment', 'reception desk', 'robot exhibit emotional behavior', 'robots dance', 'savioke labs', 'savioke labs', 'savioke team', 'short interview', 'short iteration', 'silicon valley', 'simple prototypes', 'smart machines', 'social science discipline', 'sound effects display', 'special skills', 'technological mastery — design', 'technological — it’s', 'technology companies', 'technology philosophy', 'test customers', 'tough questions', 'unique domain experience', 'user experience', 'user experience jackpot', 'user feedback', 'user reactions', 'user research', 'user study']\n",
      "========================================\n",
      "Noun_phrases in document 12\n",
      "========================================\n",
      "['% consumers', 'android smartphone', 'android users', 'apple’s chatbot', 'apple’s chatbot', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence', 'artificial intelligence ai', 'beneficial sources', 'bright future', 'bullet points', 'business use', 'chatbot imparts', 'chatbot user', 'chatbots humanistic', 'chatbots transform', 'competitive edge', 'computer programs', 'computer system', 'consumers’ online', 'current weather information whenever', 'customer service', 'customer service segment till 2020. humans', 'customer’s experience', 'daily lives', 'direct hint', 'early adopter', 'effective solutions', 'embrace chatbots', 'everyday interactions', 'exact definition', 'facebook messenger', 'facebook messenger', 'facebook messenger', 'gas stations', 'general information', 'good customer service', 'google assistant', 'grocery chatbot', 'helpful conversations', 'high level enterprises', 'hotel reservations', 'impractical dream', 'information industry', 'investment agent', 'investment plans', 'loyal friends', 'mac facebook’s chatbot facebook', 'mac google’s chatbot', 'machine learning', 'natural language processing', 'nearby restaurants', 'news chatbot', 'online businesses', 'online businesses businesses', 'order groceries', 'personal assistant', 'personal assistant', 'personal google', 'personal help chatbot', 'possible time', 'problematic situations', 'repetitive clients', 'retail industry', 'scientific definition', 'smart bot', 'social media platforms', 'superior technology', 'surprising chatbot statistics', 'term “chatbots', 'thenextweb chatbots', 'user’s experience', 'viral credit', 'way businesses interact', 'weather indicator chatbot', '” i']\n",
      "========================================\n",
      "Noun_phrases in document 13\n",
      "========================================\n",
      "['$ 584m', '2020. i’m', '5ish links', '5ish links', 'apple’s mainstream success —', 'bad problems', 'big chunk', 'big layoffs', 'big payouts', 'blockbuster launches', 'bloomberg businessweek', 'brief moment', 'brooks barnes', 'christopher palmeri', 'company doesn’t', 'company refuses', 'company’s parks division “didn’t', 'company’s perspective', 'cool novelty item', 'couple years', 'didn’t release', 'disney isn’t', 'early part', 'espn —', 'exact wrong thing', 'expensive excursion', 'fantastic ip', 'final product', 'forthcoming sequels', 'full creative control', 'good news', 'good problems', 'good quarter', 'great post', 'harry potter', 'harry potter devin leonard', 'holiday quarter', 'i don’t', 'inventory issues', 'james andrew miller', 'james bond films', 'launch quarters', 'launch snes mini', 'launch —', 'layoffs joe drape', 'main key titles', 'mario kart', 'mobile game', 'nes mini', 'nes mini', 'nes mini', 'nes mini', 'nes mini', 'nes mini shipments', 'new actual mario game', 'new consoles', 'new hit', 'new york times', 'newsletter —', 'nintendo’s core product', 'nintendo’s surface', 'on-screen folks', 'park business', 'particular —', 'permanent product', 'popular retro console', 'pro matters', 'pro situation', 'production issues', 'production run', 'professional products —', 'pure balance sheet sense… disney’s intergalactic theme park quest', 'pure incompetence', 'question they’re', 'resale market', 'rights fees', 'shouldn’t abandon', 'significant losses', 'snes mini', 'snes mini', 'snes mini', 'star wars', 'subscriber base', 'sure disney', 'surface look', 'surprising —', 'talent base.”', 'terms —', 'theme park rights', 'they’re fools', 'top-of-the-line market', 'total control', 'warner bros. entertainment inc.', 'wb/rowling’s desire', 'wii u', 'wii u', 'wild —', 'worldwide leader', 'wrong strategic things', 'year tom phillips']\n",
      "========================================\n",
      "Noun_phrases in document 14\n",
      "========================================\n",
      "['% accuracy', '% accuracy', 'alphanumeric passwords', 'app makers', 'apps need', 'battery status', 'battery status api', 'cell phone', 'concern lingers', 'corresponding phone orientation', 'dan dragomir we’ve', 'direct correlation', 'dr. maryam mehrnezhad’s team', 'dr. mehrnezhad’s team', 'four-digit pin', 'four-digit pin', 'four-digit pin number', 'four-digit pins', 'gyroscope data', 'gyroscope’s output', 'hacker sprinkles powder', 'hot topic nowadays', 'immediate answers', 'iot device', 'iot industry', 'iot security', 'iot security', 'latent fingerprints', 'latent information', 'machine learning', 'machine learning', 'machine learning algorithm', 'machine learning algorithms', 'malicious programs', 'malicious website', 'mobile view', 'newcastle university', 'phone industry —', 'phone orientation', 'phone programs', 'pin number', 'pin numbers', 'real world use —', 'research phase', 'research team', 'screen size', 'security risks', 'sensitive information', 'significant hinderances', 'silent sensors.”', 'smart sensors', 'television shows', 'theoretical hack', 'theoretical hack', 'touch actions', 'track user behavior', 'track users', 'training data', 'user touch actions', 'various firms', 'whole — dr. mehrnezhad', 'wide fragmentation', '“non-sensitive” information']\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Noun phrases are phrases that function as a noun (ie a 'thing' or concept), eg 'collaboration technology' in\n",
    "document 1 below.\n",
    "'''\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print('Noun_phrases in document {}'.format(i + 1))\n",
    "    noun_phrases = sorted(blob.noun_phrases)\n",
    "    print('========================================')\n",
    "    print(noun_phrases)\n",
    "    print('========================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a list of stopwords for use in AntConc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "with open('english-stopwords-nltk.txt', 'w') as f:\n",
    "    for word in sw:\n",
    "        f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AntConc, the stoplist can be applied via the Word List Tool Preferences dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cat english-stopwords-nltk.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading output from AntConc into your Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more than one way to do this. A good way is to save the results you want to keep from AntConc by going to File > Save Output to Text File. Put the resulting text file in the same directory as your Jupyter notebook (my example is called 'antconc_word_lst_results.txt'). Then use the following code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Play around with the number 190 (number of characters, not words in this situation)\n",
    "until you get the desired result.\n",
    "'''\n",
    "\n",
    "with open('antconc_word_lst_results.txt') as f:\n",
    "    results = f.read()\n",
    "    print(results[:190])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When displaying results from the AntConc Concordance tool, to get your results to look nice you may need to limit adjust the Concordance Tool Preferences to limit the width and columns displayed."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "nbpresent": {
   "slides": {
    "0a0b7c73-2f81-4385-8200-95696c25b960": {
     "id": "0a0b7c73-2f81-4385-8200-95696c25b960",
     "prev": "6d84d623-1b0d-4194-aaf7-d290d0552bc8",
     "regions": {
      "29ceee1b-8df7-4fd0-be29-a8f4e9d2fdf2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c7299b07-09e1-40dd-8db2-7ea852fb321e",
        "part": "whole"
       },
       "id": "29ceee1b-8df7-4fd0-be29-a8f4e9d2fdf2"
      }
     }
    },
    "6d84d623-1b0d-4194-aaf7-d290d0552bc8": {
     "id": "6d84d623-1b0d-4194-aaf7-d290d0552bc8",
     "prev": "eeefe362-d8d6-43e5-b19b-f61ba4f76357",
     "regions": {
      "526ab7de-5562-4148-a36b-6ed6cb6e9909": {
       "attrs": {
        "height": 0.8,
        "width": 0.45,
        "x": 0.5,
        "y": 0.1
       },
       "id": "526ab7de-5562-4148-a36b-6ed6cb6e9909"
      },
      "e499c5db-f296-4236-86e4-f148f8d35bc1": {
       "attrs": {
        "height": 0.8,
        "width": 0.45,
        "x": 0.05,
        "y": 0.1
       },
       "id": "e499c5db-f296-4236-86e4-f148f8d35bc1"
      }
     }
    },
    "7f2aa189-5231-4af1-b62b-8a16bef879f6": {
     "id": "7f2aa189-5231-4af1-b62b-8a16bef879f6",
     "prev": "0a0b7c73-2f81-4385-8200-95696c25b960",
     "regions": {
      "df015176-09e4-4610-89c8-a20297476e9d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "88a7c217-fb86-4a0f-b49d-d0610b5d0e4c",
        "part": "whole"
       },
       "id": "df015176-09e4-4610-89c8-a20297476e9d"
      }
     }
    },
    "eeefe362-d8d6-43e5-b19b-f61ba4f76357": {
     "id": "eeefe362-d8d6-43e5-b19b-f61ba4f76357",
     "prev": null,
     "regions": {
      "5d71254a-8efb-4c4c-8bf2-dfb1eb0fde19": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "de9a9366-4c0c-4118-b96c-031455a19885",
        "part": "whole"
       },
       "id": "5d71254a-8efb-4c4c-8bf2-dfb1eb0fde19"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
