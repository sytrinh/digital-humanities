{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Recipes for Assignment 1\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "The documentation for TextBlob, if you're interested is [here](https://textblob.readthedocs.io/en/dev/)  \n",
    "TF-IDF code adapted from Steven Loria: http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/  \n",
    "Explanations for the code below are provided. Feel free to modify the code to try out ideas, but doing so is not required for assignment 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6312bfd4c87a.txt\n",
      "bde914224749.txt\n",
      "89b97cb1cca5.txt\n",
      "38a6c08d4652.txt\n",
      "b054a3b4fb16.txt\n",
      "fa5a1fd08da4.txt\n",
      "f611db9ba568.txt\n",
      "8ebd88efcff6.txt\n",
      "2361705f299d.txt\n",
      "3ebb50ba370e.txt\n",
      "3f9d20f019a3.txt\n",
      "efca757f0313.txt\n",
      "1918ebc2ccb0.txt\n",
      "becf40fd76b9.txt\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this cell we load a directory of text files into memory using the TextBlob library.\n",
    "TextBlob is a simpler interface to many of the Natural Language Tool Kit programs.\n",
    "We're using the variable 'blob' to refer to a TextBlob object, which contains our text.\n",
    "It allows us to do useful things like blob.lower() to lowercase all words, or\n",
    "blob.words to access the words as a list.\n",
    "\n",
    "Here we assume your text files are in a directory called 'text_files'.\n",
    "The code below iterates through each filename in the 'text_files' directory,\n",
    "opens it, creates a TextBlob object, lowercases, strips commas, and saves it to a\n",
    "list of texts, here called 'bloblist'. One by one we accumulate a list of prepared texts.\n",
    "'''\n",
    "\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "\n",
    "bloblist = []\n",
    "\n",
    "def check_file(filename):\n",
    "    if not filename.endswith('.txt'):\n",
    "        return None\n",
    "    if filename.startswith('.'):\n",
    "        return None\n",
    "    return filename\n",
    "\n",
    "\n",
    "path = 'text_files/'\n",
    "for filename in os.listdir(path):\n",
    "    if check_file(filename):\n",
    "        with open(os.path.join(path, filename), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            print(filename)\n",
    "            text = f.read()\n",
    "            blob = TextBlob(text)\n",
    "            blob_lower = blob.lower()\n",
    "            blob_strip = blob_lower.strip(',')\n",
    "            bloblist.append(blob_strip)\n",
    "        \n",
    "print(len(bloblist))\n",
    "        \n",
    "# a = '''\n",
    "# In this cell we load a directory of text files into memory using the TextBlob library.\n",
    "# TextBlob is a simpler interface to many of the Natural Language Tool Kit programs.\n",
    "# We're using the variable 'blob' to refer to a TextBlob object, which contains our text.\n",
    "# It allows us to do useful things like blob.lower() to lowercase all words, or\n",
    "# blob.words to access the words as a list.\n",
    "\n",
    "# Here we assume, your text files are in a directory called 'text_files'.\n",
    "# The code below iterates through each filename in the 'text_files' directory,\n",
    "# opens it, creates a TextBlob object, lowercases, strips commas, and saves it to a\n",
    "# list of texts, here called 'bloblist'. One by one we accumulate a list of prepared texts.\n",
    "# '''\n",
    "# blob = TextBlob(a)\n",
    "# print(blob)\n",
    "# print(blob.strip(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of words in each 'blob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 :  2255\n",
      "Text 2 :  917\n",
      "Text 3 :  1110\n",
      "Text 4 :  1589\n",
      "Text 5 :  952\n",
      "Text 6 :  575\n",
      "Text 7 :  1237\n",
      "Text 8 :  460\n",
      "Text 9 :  951\n",
      "Text 10 :  984\n",
      "Text 11 :  1041\n",
      "Text 12 :  809\n",
      "Text 13 :  1225\n",
      "Text 14 :  756\n",
      "Total: 14861\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code iterates through each document (or 'blob') and uses the len() function to count how many words are in it.\n",
    "Again we use the \"accumulator pattern\" to iteratively add the number of words in each document to a running total\n",
    "variable called corpus_total_words.\n",
    "'''\n",
    "\n",
    "corpus_total_words = 0\n",
    "for i, blob in enumerate(bloblist):\n",
    "    corpus_total_words = corpus_total_words + len(blob.words)\n",
    "    print(\"Text {}\".format(i + 1), \": \", len(blob.words))\n",
    "print(\"Total:\", corpus_total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Notice the accumulator pattern is used often. We loop through our data (bloblist), creating a new data\n",
    "structure for each text (blob_frequency_dictionary). We then loop through the words of each text (blob.words)\n",
    "and find the frequency of that word in the text - ie how many times it occurs in that text - and record it. \n",
    "This blob_frequency_dictionary data looks something like: {'the': 38, 'to':33, 'and':28 ...}\n",
    "By default no stopwords are applied, but you can uncomment a line below to apply the NLTK English stopword list.\n",
    "This will filter out function words, leaving you with the 'content' or 'lexical' words that are likely to be\n",
    "of interest for this assignment.\n",
    "After this we sort by the \n",
    "'''\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    blob_frequency_dictionary = {}\n",
    "    for word in blob.words:\n",
    "        count = blob.words.count(word)\n",
    "        if word not in blob_frequency_dictionary:\n",
    "            # add stopword filtering here by uncommenting the next line, and indenting the line after it\n",
    "            # word.lower() not in sw:\n",
    "            blob_frequency_dictionary[word] = count\n",
    "        sorted_words = sorted(blob_frequency_dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "    print('10 Most frequent words in document {}'.format(i + 1))\n",
    "    for word in sorted_words[:10]:\n",
    "        print(\"\\tWord:\", word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some NLTK text objects and view concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Note your texts may not be in the order they appear as files, but you can view the list\n",
    "by running this cell.\n",
    "'''\n",
    "\n",
    "from nltk.text import Text\n",
    "\n",
    "nltk_text_list = []\n",
    "\n",
    "for blob in bloblist:\n",
    "    nltk_text = Text(blob.words)\n",
    "    nltk_text_list.append(nltk_text)\n",
    "\n",
    "nltk_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "You can select a text by changing the number 6 below - in this example 0-13 will work.\n",
    "You can also specify how many lines and how 'wide' you'd like the text snippet to be.\n",
    "'''\n",
    "\n",
    "nltk_text_list[6].concordance('bluetooth', lines=50, width=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use NLTK's similar() and common_contexts() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NLTK's similar() method finds words that appear in the same context, ie have the same words on either side.\n",
    "'''\n",
    "\n",
    "nltk_text_list[7].similar('work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NLTK's common_contexts() methods works in the opposite direction to similar().\n",
    "Give it two words that share a context (ie matching words on either side), and\n",
    "it will show you what the context words are. \n",
    "There can be multiple contexts, and this gets more interesting the larger our \n",
    "texts are.\n",
    "'''\n",
    "\n",
    "nltk_text_list[7].common_contexts(['work', 'brain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency - Inverse Document Frequency (TF-IDF) is a weighting that finds words that are characteristic of a document within a corpus. It finds words that appear quite frequently in a given document, but not in the other documents. \n",
    "\n",
    "Words that occur only once or twice in a single document and not in any other documents don't tell us a lot about the document - they may be just the whim of the writer. Similarly, words that appear a lot in all the documents don't tell us much about the differences between documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions\n",
    "\n",
    "For each word in the corpus:\n",
    "\n",
    "**Term Frequency** (tf) = frequency of the word in each document\n",
    "\n",
    "**Document Frequency** (df) = number of documents in the corpus containing the word\n",
    "\n",
    "**Inverse Document Frequency** (idf) = (logarithm of) the number of documents divided by the document frequency for the word\n",
    "\n",
    "So tf-idf for a word in the corpus is calculated by tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c7299b07-09e1-40dd-8db2-7ea852fb321e"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Here are function definitions for tf, df (here called 'n_containing), idf and tfidf.\n",
    "'''\n",
    "\n",
    "import math\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "88a7c217-fb86-4a0f-b49d-d0610b5d0e4c"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Here we loop through the list of documents called 'bloblist'.\n",
    "Scores is a dictionary of key:value pairs. \n",
    "Each key is a word in the document and the value is its tfidf score. \n",
    "Results are sorted by the tfidf score with the largest value at the top.\n",
    "Lastly we print the first 10 results for each document.\n",
    "'''\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:10]:\n",
    "        print(\"\\t{}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Noun phrases are phrases that function as a noun (ie a 'thing' or concept), eg 'collaboration technology' in\n",
    "document 1 below.\n",
    "'''\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print('Noun_phrases in document {}'.format(i + 1))\n",
    "    noun_phrases = sorted(blob.noun_phrases)\n",
    "    print('========================================')\n",
    "    print(noun_phrases)\n",
    "    print('========================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a list of stopwords for use in AntConc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "with open('english-stopwords-nltk.txt', 'w') as f:\n",
    "    for word in sw:\n",
    "        f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AntConc, the stoplist can be applied via the Word List Tool Preferences dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cat english-stopwords-nltk.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading output from AntConc into your Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more than one way to do this. A good way is to save the results you want to keep from AntConc by going to File > Save Output to Text File. Put the resulting text file in the same directory as your Jupyter notebook (my example is called 'antconc_word_lst_results.txt'). Then use the following code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Play around with the number 190 (number of characters, not words in this situation)\n",
    "until you get the desired result.\n",
    "'''\n",
    "\n",
    "with open('antconc_word_lst_results.txt') as f:\n",
    "    results = f.read()\n",
    "    print(results[:190])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When displaying results from the AntConc Concordance tool, to get your results to look nice you may need to limit adjust the Concordance Tool Preferences to limit the width and columns displayed."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "nbpresent": {
   "slides": {
    "0a0b7c73-2f81-4385-8200-95696c25b960": {
     "id": "0a0b7c73-2f81-4385-8200-95696c25b960",
     "prev": "6d84d623-1b0d-4194-aaf7-d290d0552bc8",
     "regions": {
      "29ceee1b-8df7-4fd0-be29-a8f4e9d2fdf2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c7299b07-09e1-40dd-8db2-7ea852fb321e",
        "part": "whole"
       },
       "id": "29ceee1b-8df7-4fd0-be29-a8f4e9d2fdf2"
      }
     }
    },
    "6d84d623-1b0d-4194-aaf7-d290d0552bc8": {
     "id": "6d84d623-1b0d-4194-aaf7-d290d0552bc8",
     "prev": "eeefe362-d8d6-43e5-b19b-f61ba4f76357",
     "regions": {
      "526ab7de-5562-4148-a36b-6ed6cb6e9909": {
       "attrs": {
        "height": 0.8,
        "width": 0.45,
        "x": 0.5,
        "y": 0.1
       },
       "id": "526ab7de-5562-4148-a36b-6ed6cb6e9909"
      },
      "e499c5db-f296-4236-86e4-f148f8d35bc1": {
       "attrs": {
        "height": 0.8,
        "width": 0.45,
        "x": 0.05,
        "y": 0.1
       },
       "id": "e499c5db-f296-4236-86e4-f148f8d35bc1"
      }
     }
    },
    "7f2aa189-5231-4af1-b62b-8a16bef879f6": {
     "id": "7f2aa189-5231-4af1-b62b-8a16bef879f6",
     "prev": "0a0b7c73-2f81-4385-8200-95696c25b960",
     "regions": {
      "df015176-09e4-4610-89c8-a20297476e9d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "88a7c217-fb86-4a0f-b49d-d0610b5d0e4c",
        "part": "whole"
       },
       "id": "df015176-09e4-4610-89c8-a20297476e9d"
      }
     }
    },
    "eeefe362-d8d6-43e5-b19b-f61ba4f76357": {
     "id": "eeefe362-d8d6-43e5-b19b-f61ba4f76357",
     "prev": null,
     "regions": {
      "5d71254a-8efb-4c4c-8bf2-dfb1eb0fde19": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "de9a9366-4c0c-4118-b96c-031455a19885",
        "part": "whole"
       },
       "id": "5d71254a-8efb-4c4c-8bf2-dfb1eb0fde19"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
